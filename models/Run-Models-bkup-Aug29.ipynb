{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxZiI7xcrT4B"
      },
      "source": [
        "# Run Models (includes RandomForest and XGBoost)\n",
        "\n",
        "Documentation\n",
        "https://model.earth/RealityStream  \n",
        "https://model.earth/RealityStream/input/industries backed-up to Run-Models-bkup.ipynb\n",
        "\n",
        "DONE Aashish: Used Pandas for integrated_df (became df) for save_training = False.  \n",
        "DONE Loren: Loaded parameters.yaml and saved locally for customization.  \n",
        "https://chatgpt.com/share/e4a2ee73-ab74-4551-9868-37b9b5b6b359  \n",
        "\n",
        "**TO DO:** Test default pathes in CoLab. Allow these incoming parameters to be deleted (by editing the right side panel) so default paths in the CoLab can be applied: features.data, features.path, targets.data, targets.path. The defaults will be industry features and the bee population targets dataset.\n",
        "\n",
        "TO DO: Using the local directory for saving training data has a bug. Ivy wrote: I think it is not reading the CSV, so there is nothing to concatenate to create the dataframe for training. Currently ussing the backup in github for development.\n",
        "\n",
        "**Sai:** ValueError: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, The experimental DMatrix parameter`enable_categorical` must be set to `True`.  \n",
        "\n",
        "We have newer data to solve this:\n",
        "Invalid columns:Population-2018: object, Population-2019: object, Population-2020: object\n",
        "\n",
        "TO DO: Test that default target path for bee data works by deleting in left panel after pullin in parameters.yaml. Then test that panels 15 and 16 work.  \n",
        "if param.targets.path: # Override with value from yaml  \n",
        "    target_url = param.targets.path\n",
        "\n",
        "target_df got problems.\n",
        "\n",
        "TO DO: Pull 2-column target zip code UN topics directly from Google Data Commons based on para\n",
        "\n",
        "DONE Ivy: In the same panel as each accuracy report, call a new function called displayModelHeader to display the model name (as a bold header) and the file paths for features and targets above the report.\n",
        "\n",
        "DONE Ivy: Show the parameter values below each path at the top of each accuracy report. So under the Feature path we'd have:  \n",
        "startyear: 2017, endyear: 2021, naics: [6], state: ME\n",
        "\n",
        "DONE Lily: Add support for multiple states. After running the third panel, you can edit the custom yaml on the right to set state: CT, ME, MA, NH, RI, VT.  Then add a loop that runs when there are multiple states. We'll add a file called parameters-new-england.yaml in the root of the RealityStream repo with the six states as features.states. Load here and add python to loop through the states.\n",
        "\n",
        "TO DO: Add more parameters.yaml files that pull features/targets and join on the county Fips column. Add a path parameter that pulls from \"all-years\" which are generated by our [Industry Features CoLab](https://colab.research.google.com/drive/1HJnuilyEFjBpZLrgxDa4S0diekwMeqnh?usp=sharing). All years on GitHub:  \n",
        "https://github.com/ModelEarth/community-timelines/tree/main/training/all-years\n",
        "(These were created by Ronan)\n",
        "\n",
        "TO DO: Load blinks/parameters-blinks.yaml and use target.column to limit to y column!\n",
        "\n",
        "TO DO: Load 1 of these 4 bee targets using parameters.yaml setting.\n",
        "https://github.com/ModelEarth/RealityStream/tree/main/input/bees/targets  \n",
        "\n",
        "TO DO: Setting model_name = \"XGBoost\" resulted in the error:\n",
        "\n",
        "Akshay: Load parameters.yaml into Streamlit\n",
        "https://reality.streamlit.app/?parameters=https://raw.githubusercontent.com/ModelEarth/RealityStream/main/parameters.yaml\n",
        "\n",
        "TO DO: Load common util files (from GitHub) into both Google CoLab and RealityStream Streamlit app.py.\n",
        "\n",
        "TO DO: Avoid sorting incoming parameters.yaml alphabetically. Attempt using  OrderedDict is commented out is several places below.\n",
        "\n",
        "TO DO: Only import models requested by parameters.yaml. Move \"from sklearn\" imports to step after parameters are edited in side panel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtFKCS4LkOzN"
      },
      "outputs": [],
      "source": [
        "# If save_training=True your files will reside under the folder to the left\n",
        "save_training = True  # When False, Pandas is used.\n",
        "\n",
        "import pandas as pd\n",
        "import regex as re\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import yaml\n",
        "import requests\n",
        "#from collections import OrderedDict # Effort to retain incoming yaml order rather than alphabetizing.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_curve, roc_auc_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import xgboost as xgb\n",
        "from xgboost import plot_importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "LRqQNpac3jeA",
        "outputId": "894c4974-724f-4234-e57f-4f662ae022a4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'OrderedDict' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-c4227be14b08>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Not in use yet. To avoid alphabetizing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mordered_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSafeLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_pairs_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mOrderedLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'OrderedDict' is not defined"
          ]
        }
      ],
      "source": [
        "# Default parameters file and local path to save at.\n",
        "# After running you can edit parameters that appear to the right.\n",
        "# Coming soon:\n",
        "# You can change the bees year in the targets.path to: 2007, 2012, 1017, 2022\n",
        "# TO DO: Changing the year in the bees target does not work yet. Make updates to other panels.\n",
        "parametersSource = \"https://raw.githubusercontent.com/ModelEarth/RealityStream/main/parameters.yaml\"\n",
        "\n",
        "# Blinks - Under development by Kelvin\n",
        "# parametersSource = \"https://raw.githubusercontent.com/ModelEarth/RealityStream/main/input/blinks/parameters-blinks.yaml\"\n",
        "\n",
        "importNewParameters = True\n",
        "overwriteExistingParameter = False\n",
        "localParametersPath = '/content/parametersLocal.yaml'\n",
        "\n",
        "# Not in use yet. To avoid alphabetizing.\n",
        "def ordered_load(stream, Loader=yaml.SafeLoader, object_pairs_hook=OrderedDict):\n",
        "    class OrderedLoader(Loader):\n",
        "        pass\n",
        "    def construct_mapping(loader, node):\n",
        "        loader.flatten_mapping(node)\n",
        "        return object_pairs_hook(loader.construct_pairs(node))\n",
        "    OrderedLoader.add_constructor(\n",
        "        yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG,\n",
        "        construct_mapping)\n",
        "    return yaml.load(stream, OrderedLoader)\n",
        "\n",
        "# Fetch the parameters from the source URL\n",
        "response = requests.get(parametersSource)\n",
        "parametersSourceData = yaml.safe_load(response.content)\n",
        "#parametersSourceData = ordered_load(response.content)\n",
        "\n",
        "# Function to merge dictionaries\n",
        "def merge_dicts(source, local, import_new, overwrite_existing):\n",
        "    for key, value in source.items():\n",
        "        if key in local:\n",
        "            if isinstance(value, dict) and isinstance(local[key], dict):\n",
        "                merge_dicts(value, local[key], import_new, overwrite_existing)\n",
        "            elif overwrite_existing:\n",
        "                local[key] = value\n",
        "        else:\n",
        "            if import_new:\n",
        "                local[key] = value\n",
        "\n",
        "class DictToObject:\n",
        "    def __init__(self, dictionary):\n",
        "        for key, value in dictionary.items():\n",
        "            if isinstance(value, dict):\n",
        "                value = DictToObject(value)\n",
        "            self.__dict__[key] = value\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        return self.__dict__[key]\n",
        "\n",
        "    def __setitem__(self, key, value):\n",
        "        self.__dict__[key] = value\n",
        "\n",
        "# Load local parameters if they exist\n",
        "if os.path.exists(localParametersPath):\n",
        "    with open(localParametersPath, 'r') as file:\n",
        "        parametersLocalData = yaml.safe_load(file)\n",
        "        #parametersLocalData = ordered_load(file)\n",
        "else:\n",
        "    parametersLocalData = {}\n",
        "    #parametersLocalData = OrderedDict()\n",
        "\n",
        "# Merge parameters according to specified rules\n",
        "merge_dicts(parametersSourceData, parametersLocalData, importNewParameters, overwriteExistingParameter)\n",
        "\n",
        "# Save the merged parameters locally\n",
        "with open(localParametersPath, 'w') as file:\n",
        "    yaml.dump(parametersLocalData, file)\n",
        "\n",
        "# Display local parameters file in the left side of Colab\n",
        "from google.colab import files\n",
        "files.view(localParametersPath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "alauCxr5yHF7",
        "outputId": "5a4bac07-1639-4c8d-c5be-0fed11394d4c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/parametersLocal.yaml'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d61a1d53b343>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load parameters from the local file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocalParametersPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mparam_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/parametersLocal.yaml'"
          ]
        }
      ],
      "source": [
        "# Apply Parameters\n",
        "# Load local parameters and print below.\n",
        "\n",
        "import yaml\n",
        "\n",
        "localParametersPath = '/content/parametersLocal.yaml'\n",
        "\n",
        "# Load parameters from the local file\n",
        "with open(localParametersPath, 'r') as file:\n",
        "    param_dict = yaml.safe_load(file)\n",
        "\n",
        "# Convert dictionary to an object with dot notation access\n",
        "param = DictToObject(param_dict)\n",
        "\n",
        "# Print the parameters\n",
        "def print_param(obj, indent=0):\n",
        "    for key in obj.__dict__.keys():\n",
        "        value = getattr(obj, key)\n",
        "        if isinstance(value, DictToObject):\n",
        "            print(' ' * indent + f\"{key}:\")\n",
        "            print_param(value, indent + 2)\n",
        "        else:\n",
        "            print(' ' * indent + f\"{key}: {value}\")\n",
        "\n",
        "# Also not in use yet, these will only be used if parametersLocal.yaml omits them.\n",
        "features_data = param['features']['data'] # \"industries\"\n",
        "features_path = param['features']['path'] # \"https://raw.githubusercontent.com/ModelEarth/community-timelines/main/training/naics{naics}/US/counties/{year}/US-{state}-training-naics{naics}-counties-{year}.csv\"\n",
        "targets_data  = param['targets']['data'] # \"bees\"\n",
        "targets_path  = param['targets']['path'] # \"https://raw.githubusercontent.com/ModelEarth/RealityStream/main/input/bees/targets/bees-targets-increase2022.csv\"\n",
        "\n",
        "states=param.features.state\n",
        "states=states.replace(\" \",\"\").split(\",\")\n",
        "start_year=param.features.startyear\n",
        "end_year=param.features.endyear\n",
        "years=range(start_year,end_year+1)\n",
        "naics_levels=param.features.naics\n",
        "\n",
        "############Lily#########param_short_name##############\n",
        "if param.targets.data in [\"bees\",\"input/bees/targets\",\"input/bees/targets/bees-targets.csv\",\"https://raw.githubusercontent.com/ModelEarth/RealityStream/main/input/bees/targets/bees-targets.csv\"]:\n",
        "  param.targets.path=\"https://raw.githubusercontent.com/ModelEarth/RealityStream/main/input/bees/targets/bees-targets.csv\"\n",
        "\n",
        "\n",
        "print(param.targets.path)\n",
        "if \"/\" not in param.targets.data and \" \" in param.targets.data:\n",
        "  temp_list=param.targets.data.split()\n",
        "  param.targets.path=f\"https://raw.githubusercontent.com/ModelEarth/RealityStream/main/input/bees/targets/{temp_list[0]}-targets-{temp_list[1]}.csv\"\n",
        "\n",
        "print(param.targets.path)\n",
        "#if not param.models:\n",
        "# param.models=[\"rbf\"]\n",
        "\n",
        "#if param.models==\"all\":\n",
        "#  param.models=[\"svc\", \"rfc\", \"lr\", \"rbf\", \"location-forest\"]\n",
        "\n",
        "if not param.models:\n",
        "  param.models=\"rbf\"\n",
        "\n",
        "if param.models==\"all\":\n",
        "  param.models=\"svc, rfc, lr, rbf, location-forest\"\n",
        "\n",
        "print(param.models)\n",
        "\n",
        "\n",
        "############################################\n",
        "def format_url(url,naics,year,state):\n",
        "  import re\n",
        "  pattern = r\"{([a-zA-Z]+)}\"\n",
        "  matches = re.findall(pattern, url)\n",
        "  replacements = {\"naics\": naics, \"year\": year, \"state\": state}\n",
        "  formatted_url = url.format(**replacements)\n",
        "  return formatted_url\n",
        "feature_paths={}\n",
        "for state in states:\n",
        "  for year in years:\n",
        "    for naics in naics_levels:\n",
        "      print(f\"\\nnaics: {naics}, year: {year}, state: {state}\")\n",
        "      new_path=format_url(param.features.path,naics,year,state)\n",
        "      print(\"param.features.path:\", new_path)\n",
        "      if state not in feature_paths:\n",
        "        feature_paths[state] = [new_path]\n",
        "      else:\n",
        "        feature_paths[state].append(new_path)\n",
        "      print(\"param.targets.path:\", param.targets.path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIth7Fegr26z"
      },
      "outputs": [],
      "source": [
        "feature_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z12cWU4y09on"
      },
      "outputs": [],
      "source": [
        "# TO DO: Setting model_name = \"XGBoost\" resulted in the error:\n",
        "# ValueError: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, The experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:Population-2018: object, Population-2019: object, Population-2020: object\n",
        "\n",
        "# TO DO: These are in use, replace with parameters\n",
        "\n",
        "dataset_name = \"bees\"  # TO DO: eliminate since features and targets will differ.\n",
        "model_name = \"RandomForest\"  # Specify the model to be trained\n",
        "all_model_list = [\"LogisticRegression\", \"SVM\", \"MLP\", \"RandomForest\", \"XGBoost\"]  # All usable models\n",
        "assert model_name in all_model_list\n",
        "valid_report_list = [\"RandomForest\", \"XGBoost\"]  # All valid models to generate feature-importance report\n",
        "\n",
        "random_state = 42  # Specify random state\n",
        "\n",
        "# Feature related information:\n",
        "country = \"US\"\n",
        "years = range(2017, 2022)\n",
        "naics_level = 2\n",
        "naics_list = [2, 4, 6]\n",
        "assert naics_level in naics_list\n",
        "# Target related information:\n",
        "target_url = f\"https://raw.githubusercontent.com/ModelEarth/RealityStream/main/input/{dataset_name}/targets/{dataset_name}-targets.csv\"\n",
        "if param.targets.path: # Override with value from yaml\n",
        "  target_url = param.targets.path\n",
        "target_df = pd.read_csv(target_url)  # Get the target csv\n",
        "\n",
        "# Originally from bee data. We might delete this.\n",
        "#target_list = [] # Later add shorthand path from parameters.yaml\n",
        "#target_column = [] # Later add shorthand path from parameters.yaml\n",
        "#year_list = [\"2002\", \"2007\", \"2012\", \"2017\", \"2022\"]\n",
        "#drop_list = ['Unnamed: 0', 'Name', 'State', 'State ANSI', 'County ANSI', \"Ag District\", \"Ag District Code\"]\n",
        "#all_drop_list = drop_list + target_list + year_list  # Drop all columns that can affect the training procedure or are not related\n",
        "\n",
        "feature_start_idx = 3  # Specify the starting column index in dataset csv for features, where first few columns are for target and id related stuff\n",
        "target_idx = 0  # Specify the column index for target\n",
        "\n",
        "#Directory Information:\n",
        "merged_save_dir = f\"../process/{dataset_name}/states-{target_column}-{dataset_name}\"  # Specify the saving dir for state-separate dataset\n",
        "full_save_dir = f\"../output/{dataset_name}/training\"  # Specify the saving dir for the integrated dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NFzT-votQRv"
      },
      "outputs": [],
      "source": [
        "target_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdUt24w63WDa"
      },
      "outputs": [],
      "source": [
        "# STEP: Get Dictionaries for states and industries\n",
        "\n",
        "# New - not yet tested. Try including DC and US Territories\n",
        "STATE_DICT = {\n",
        "    \"AL\": \"Alabama\", \"AK\": \"Alaska\", \"AZ\": \"Arizona\", \"AR\": \"Arkansas\", \"CA\": \"California\", \"CO\": \"Colorado\",\n",
        "    \"CT\": \"Connecticut\", \"DE\": \"Delaware\", \"FL\": \"Florida\", \"GA\": \"Georgia\", \"HI\": \"Hawaii\", \"ID\": \"Idaho\",\n",
        "    \"IL\": \"Illinois\", \"IN\": \"Indiana\", \"IA\": \"Iowa\", \"KS\": \"Kansas\", \"KY\": \"Kentucky\", \"LA\": \"Louisiana\",\n",
        "    \"ME\": \"Maine\", \"MD\": \"Maryland\", \"MA\": \"Massachusetts\", \"MI\": \"Michigan\", \"MN\": \"Minnesota\", \"MS\": \"Mississippi\",\n",
        "    \"MO\": \"Missouri\", \"MT\": \"Montana\", \"NE\": \"Nebraska\", \"NV\": \"Nevada\", \"NH\": \"New Hampshire\", \"NJ\": \"New Jersey\",\n",
        "    \"NM\": \"New Mexico\", \"NY\": \"New York\", \"NC\": \"North Carolina\", \"ND\": \"North Dakota\", \"OH\": \"Ohio\", \"OK\": \"Oklahoma\",\n",
        "    \"OR\": \"Oregon\", \"PA\": \"Pennsylvania\", \"RI\": \"Rhode Island\", \"SC\": \"South Carolina\", \"SD\": \"South Dakota\",\n",
        "    \"TN\": \"Tennessee\", \"TX\": \"Texas\", \"UT\": \"Utah\", \"VT\": \"Vermont\", \"VA\": \"Virginia\", \"WA\": \"Washington\",\n",
        "    \"WV\": \"West Virginia\", \"WI\": \"Wisconsin\", \"WY\": \"Wyoming\",\n",
        "    # \"DC\": \"District of Columbia\",\n",
        "    # US Territories\n",
        "    # \"AS\": \"American Samoa\", \"GU\": \"Guam\", \"MP\": \"Northern Mariana Islands\", \"PR\": \"Puerto Rico\", \"VI\": \"U.S. Virgin Islands\"\n",
        "}\n",
        "\n",
        "STATE_DICT_DELETE = {\n",
        "    \"AL\": \"ALABAMA\",\"AK\": \"ALASKA\",\"AZ\": \"ARIZONA\",\"AR\": \"ARKANSAS\",\"CA\": \"CALIFORNIA\",\"CO\": \"COLORADO\",\"CT\": \"CONNECTICUT\",\"DE\": \"DELAWARE\",\"FL\": \"FLORIDA\",\"GA\": \"GEORGIA\",\"HI\": \"HAWAII\",\"ID\": \"IDAHO\",\"IL\": \"ILLINOIS\",\"IN\": \"INDIANA\",\"IA\": \"IOWA\",\"KS\": \"KANSAS\",\"KY\": \"KENTUCKY\",\"LA\": \"LOUISIANA\",\"ME\": \"MAINE\",\"MD\": \"MARYLAND\",\"MA\": \"MASSACHUSETTS\",\"MI\": \"MICHIGAN\",\"MN\": \"MINNESOTA\",\"MS\": \"MISSISSIPPI\",\"MO\": \"MISSOURI\",\"MT\": \"MONTANA\",\"NE\": \"NEBRASKA\",\"NV\": \"NEVADA\",\"NH\": \"NEW HAMPSHIRE\",\"NJ\": \"NEW JERSEY\",\"NM\": \"NEW MEXICO\",\"NY\": \"NEW YORK\",\"NC\": \"NORTH CAROLINA\",\"ND\": \"NORTH DAKOTA\",\"OH\": \"OHIO\",\"OK\": \"OKLAHOMA\",\"OR\": \"OREGON\",\"PA\": \"PENNSYLVANIA\",\"RI\": \"RHODE ISLAND\",\"SC\": \"SOUTH CAROLINA\",\"SD\": \"SOUTH DAKOTA\",\"TN\": \"TENNESSEE\",\"TX\": \"TEXAS\",\"UT\": \"UTAH\",\"VT\": \"VERMONT\",\"VA\": \"VIRGINIA\",\"WA\": \"WASHINGTON\",\"WV\": \"WEST VIRGINIA\",\"WI\": \"WISCONSIN\",\"WY\": \"WYOMING\"\n",
        "}\n",
        "try:\n",
        "    industries_df = pd.read_csv(f\"https://raw.githubusercontent.com/ModelEarth/community-data/master/{country.lower()}/id_lists/naics{naics_level}.csv\",header=None)\n",
        "    INDUSTRIES_DICT = industries_df.set_index(0).to_dict()[1]\n",
        "except:\n",
        "    INDUSTRIES_DICT = dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MwzUBj6kPq2"
      },
      "outputs": [],
      "source": [
        "#Not using, we'll load .py from a GitHub link instead.\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jv_AUQwjnrkN"
      },
      "outputs": [],
      "source": [
        "# STEP: Create Functions\n",
        "def rename_columns(df, year):\n",
        "    rename_mapping = {}\n",
        "    for column in df.columns:\n",
        "      if column not in df.columns[:2]:\n",
        "          new_column_name = column + f'-{year}'\n",
        "          rename_mapping[column] = new_column_name\n",
        "    df.rename(columns=rename_mapping, inplace=True)\n",
        "\n",
        "def check_directory(directory_path): # Check whether the given directory exists, if not, then create it\n",
        "    if not os.path.exists(directory_path):\n",
        "        try:\n",
        "            os.makedirs(directory_path)\n",
        "            print(f\"Directory '{directory_path}' created successfully.\")\n",
        "        except OSError as e:\n",
        "            print(f\"Error creating directory '{directory_path}': {e}\")\n",
        "    else:\n",
        "        print(f\"Directory '{directory_path}' already exists.\")\n",
        "    return directory_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kylqN5dnbHPe"
      },
      "outputs": [],
      "source": [
        "target_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoPB-9qx1n7O"
      },
      "outputs": [],
      "source": [
        "# STEP: Merge feature and target data\n",
        "# If save_training=True, your files will reside in the \"process\" folder to the left.\n",
        "# Hit the refresh icon above your folder list to the left.\n",
        "if save_training:\n",
        "    save_dir = merged_save_dir  # Save in the local directory if save_training is True\n",
        "\n",
        "check_directory(save_dir)\n",
        "\n",
        "# State-separately, for each state, merging industry features and target on Fips value and County Name, return the merged csv\n",
        "############ change to feature_paths!!!!!!!!!!!!!!!!!!!!\n",
        "\n",
        "\n",
        "for state in feature_paths:\n",
        "  data=[]\n",
        "  for i,url in enumerate(feature_paths[state]):\n",
        "    df=pd.read_csv(url)\n",
        "    rename_columns(df,start_year+i)\n",
        "    data.append(df)\n",
        "    #display(df)\n",
        "  merged_df_feature = pd.merge(data[0], data[1], on=['Fips', 'Name'], how='inner')\n",
        "  for df in data[2:]:\n",
        "      merged_df_feature = pd.merge(merged_df_feature, df, on=['Fips', 'Name'], how='inner')\n",
        "  cols = merged_df_feature.columns.tolist()\n",
        "  cols = cols[:2] + sorted(cols[2:])\n",
        "  merged_df_feature = merged_df_feature[cols].rename(columns={\"Name\": \"County\"})\n",
        "  #display(merged_df_feature)\n",
        "  target_df_state=target_df[target_df.Fips.isin(merged_df_feature.Fips)]\n",
        "  #display(target_df_state)\n",
        "  merged_df = pd.merge(target_df_state,merged_df_feature, on=[\"Fips\"], how=\"inner\")\n",
        "  display(merged_df)\n",
        "  d={}\n",
        "  for k in merged_df.columns:\n",
        "    print(k)\n",
        "    ty=merged_df[k].dtypes\n",
        "    if ty not in d:\n",
        "      d[ty]=[k]\n",
        "    else:\n",
        "      d[ty].append(k)\n",
        "    print(d)\n",
        "  break\n",
        "'''\n",
        "for state in [\"ME\"]:\n",
        "    data = {}\n",
        "    for year in years:\n",
        "        url = f\"https://raw.githubusercontent.com/ModelEarth/community-timelines/main/training/naics{naics_level}/{country}/counties/{year}/{country}-{state}-training-naics{naics_level}-counties-{year}.csv\"\n",
        "        data[year] = pd.read_csv(url)\n",
        "        rename_columns(data[year], year)\n",
        "\n",
        "    merged_df_feature = pd.merge(data[2017], data[2018], on=['Fips', 'Name'], how='inner')\n",
        "    for year in range(2019, 2022):\n",
        "        merged_df_feature = pd.merge(merged_df_feature, data[year], on=['Fips', 'Name'], how='inner')\n",
        "    cols = merged_df_feature.columns.tolist()\n",
        "    cols = cols[:2] + sorted(cols[2:])\n",
        "    merged_df_feature = merged_df_feature[cols].rename(columns={\"Name\": \"County\"})\n",
        "\n",
        "    merged_df = pd.merge(merged_df_feature, target_df[target_df[\"State\"] == STATE_DICT[state]], on=[\"Fips\", \"County\"], how=\"inner\")\n",
        "    merged_df.drop(columns=all_drop_list, axis=1, inplace=True)\n",
        "\n",
        "    target = merged_df.iloc[:, -1]\n",
        "    merged_df.drop(columns=[target_column], axis=1, inplace=True)\n",
        "    merged_df.insert(0, 'target', target)\n",
        "\n",
        "    merged_df.to_csv(os.path.join(merged_save_dir, f\"{state}-{target_column}-{dataset_name}.csv\"), index=False)\n",
        "\n",
        "    if save_training:\n",
        "      save_dir = merged_save_dir #Use the local directory if not in Google Colab\n",
        "      file_path = os.path.join(save_dir, f\"{state}-{target_column}-{dataset_name}.csv\")\n",
        "      merged_df.to_csv(file_path, index=False)\n",
        "      print(f\"Saved file at: {file_path}\")\n",
        "\n",
        "      # try:\n",
        "      #   from google.colab import drive\n",
        "      #   drive.mount('/content/drive')\n",
        "      #   save_dir = '/content/drive/My Drive/RunModels' #Your Google Drive path\n",
        "      #   check_directory(save_dir)\n",
        "\n",
        "      # except ImportError:\n",
        "      #   save_dir = merged_save_dir #Use the local directory if not in Google Colab\n",
        "\n",
        "      # file_path = os.path.join(save_dir, f\"{state}-{target_column}-{dataset_name}.csv\")\n",
        "      # merged_df.to_csv(file_path, index=False)\n",
        "      # print(f\"Saved file at: {file_path}\")\n",
        "\n",
        "      merged_df.to_csv(os.path.join(merged_save_dir, f\"{state}-{target_column}-{dataset_name}.csv\"), index=False)\n",
        "\n",
        "if not save_training:\n",
        "      print(f\"Since save_training is false no files are currently saved.\")\n",
        "'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kE1xVYAbiuwQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7rpuU75xWKp"
      },
      "outputs": [],
      "source": [
        "# STEP: Integrate separate state data into one, return the full dataset csv\n",
        "# If save_training=True, your files will reside in the \"output\" folder to the left.\n",
        "# Hit the refresh icon above your folder list to the left.\n",
        "save_dir = full_save_dir  # Use the local directory if save_training is True\n",
        "\n",
        "check_directory(save_dir)\n",
        "\n",
        "dataframes = []\n",
        "csv_directory = f\"../process/{dataset_name}/states-{target_column}-{dataset_name}\"\n",
        "csv_files = os.listdir(csv_directory)\n",
        "for csv_file in csv_files:\n",
        "    if csv_file.endswith('.csv'):\n",
        "        dataframes.append(pd.read_csv(os.path.join(csv_directory, csv_file)))\n",
        "\n",
        "integrated_df = pd.concat(dataframes, ignore_index=True)\n",
        "df = integrated_df\n",
        "\n",
        "if save_training:\n",
        "  save_dir = full_save_dir #Use the local directory if not in Google Colab\n",
        "  file_path = os.path.join(save_dir, f\"{target_column}-{dataset_name}.csv\")\n",
        "  integrated_df.to_csv(file_path, index=False)\n",
        "  print(f\"Saved file at: {file_path}\")\n",
        "    # try:\n",
        "    #   from google.colab import drive\n",
        "    #   drive.mount('/content/drive', force_remount=True)\n",
        "    #   save_dir = '/content/drive/My Drive/RunModels' #Your Google Drive path\n",
        "    #   check_directory(save_dir)\n",
        "    # except ImportError:\n",
        "    #   save_dir = full_save_dir #Use the local directory if not in Google Colab\n",
        "\n",
        "  file_path = os.path.join(save_dir, f\"{target_column}-{dataset_name}.csv\")\n",
        "  integrated_df.to_csv(file_path, index=False)\n",
        "  print(f\"Saved file at: {file_path}\")\n",
        "  #integrated_df.to_csv(os.path.join(full_save_dir, f\"{target_column}-{dataset_name}.csv\"), index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SxJJr6-x9gO"
      },
      "outputs": [],
      "source": [
        "#Display bald model header on top of the report including model name and feature/target path\n",
        "def displayModelHeader(featurePath,targetPath, model):\n",
        "  print(f\"\\033[1mModel: {model}\\033\\nFeature path:{featurePath}\\nTarget path: {targetPath}\")\n",
        "  print(f\"startyear: {param.features.startyear}, endyear: {param.features.endyear}, naics:{param.features.naics}, state: {param.features.state}\")\n",
        "\n",
        "# Train the model and get the test report\n",
        "def train_model(model, X_train, y_train, X_test, y_test, over_sample):\n",
        "\n",
        "    if over_sample:\n",
        "        sm = SMOTE(random_state = 2)\n",
        "        X_train, y_train = sm.fit_resample(X_train, y_train.ravel())\n",
        "        print(\"Oversampling Done for Training Data.\")\n",
        "\n",
        "\n",
        "    model = model.fit(X_train, y_train)\n",
        "    print(\"Model Fitted Successfully.\")\n",
        "\n",
        "    # calculating y_pred\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_prob = model.predict_proba(X_test)\n",
        "\n",
        "    #roc_auc score\n",
        "    roc_auc = round(roc_auc_score(y_test, y_pred_prob[:, 1]), 2)\n",
        "    print(f\"\\033[1mROC-AUC Score\\033[0m \\t\\t: {roc_auc*100} %\")\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob[:,1], pos_label=1)\n",
        "\n",
        "    gmeans = np.sqrt(tpr * (1-fpr))\n",
        "\n",
        "    ix = np.argmax(gmeans)\n",
        "\n",
        "    print('\\033[1mBest Threshold\\033[0m \\t\\t: %.3f \\n\\033[1mG-Mean\\033[0m \\t\\t\\t: %.3f' % (thresholds[ix], gmeans[ix]))\n",
        "    best_threshold_num = round(thresholds[ix], 3)\n",
        "\n",
        "    gmeans_num = round(gmeans[ix], 3)\n",
        "\n",
        "    y_pred = (y_pred > thresholds[ix])\n",
        "\n",
        "    #ccuracy score\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_num = f\"{accuracy * 100:.1f}\"\n",
        "\n",
        "    print(\"\\033[1mModel Accuracy\\033[0m \\t\\t:\", round(accuracy,2,)*100, \"%\")\n",
        "    print(\"\\033[1m\\nClassification Report:\\033[0m\")\n",
        "\n",
        "    #Generate classification report for display and in dictionary for future report generation\n",
        "    cfc_report = classification_report(y_test, y_pred)\n",
        "    cfc_report_dict = classification_report(y_test, y_pred, output_dict= True)\n",
        "    print(cfc_report)\n",
        "\n",
        "    return model, y_pred, accuracy_num, gmeans_num, accuracy_num, roc_auc, best_threshold_num, cfc_report_dict\n",
        "\n",
        "# Train the specified model, impute the nan values, and save the trained model as well as the feature-target report\n",
        "def train(featurePath,targetPath, model_name, target_column, dataset_name, X_train, y_train, X_test, y_test, report_gen, all_model_list, valid_report_list, over_sample=False, model_saving=True, random_state=42):\n",
        "    assert model_name in all_model_list\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_train_imputed = imputer.fit_transform(X_train)\n",
        "    X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "    if model_name == \"LogisticRegression\":\n",
        "        model = LogisticRegression(max_iter=10000, random_state=random_state)\n",
        "    elif model_name == \"SVM\":\n",
        "        model = SVC(random_state=random_state,probability=True)\n",
        "    elif model_name == \"MLP\":\n",
        "        model = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=1000, random_state=random_state)\n",
        "    elif model_name == \"RandomForest\":\n",
        "        model = RandomForestClassifier(n_jobs=3, n_estimators=1000, criterion=\"gini\", random_state=random_state)\n",
        "        model_fullname = \"Random Forest\"\n",
        "    elif model_name == \"XGBoost\":\n",
        "        model = xgb.XGBClassifier(random_state=random_state, enable_categorical=True)\n",
        "        model_fullname = \"XGBoost\"\n",
        "    else:\n",
        "        raise Exception\n",
        "\n",
        "    displayModelHeader(featurePath,targetPath,model_fullname)\n",
        "    if model_name == \"XGBoost\":\n",
        "\n",
        "        model, y_pred, accuracy_num, gmeans_num, accuracy_num, roc_auc, best_threshold_num, cfc_report_dict = train_model(model, X_train, y_train, X_test, y_test, over_sample) # No need to impute nan values for XGBoost\n",
        "\n",
        "    else:\n",
        "\n",
        "        model, y_pred, accuracy_num, gmeans_num, accuracy_num, roc_auc, best_threshold_num, cfc_report_dict = train_model(model, X_train_imputed, y_train, X_test_imputed, y_test, over_sample)\n",
        "\n",
        "\n",
        "    save_dir = f\"../output/{dataset_name}/saved\"\n",
        "    check_directory(save_dir)\n",
        "\n",
        "    if model_saving:\n",
        "        if model_name == \"XGBoost\":\n",
        "            save_model(model, None, target_column, dataset_name, model_name, save_dir) # No need to impute nan values for XGBoost\n",
        "        else:\n",
        "            save_model(model, imputer, target_column, dataset_name, model_name, save_dir)\n",
        "\n",
        "    if report_gen:\n",
        "        if model_name in valid_report_list:\n",
        "            if model_name == \"RandomForest\":\n",
        "                importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': model.feature_importances_})\n",
        "                report = importance_df.sort_values(by='Importance', ascending=False)\n",
        "            elif model_name == \"XGBoost\":\n",
        "                importance_df = pd.DataFrame(list(model.get_booster().get_score().items()), columns=[\"Feature\",\"Importance\"])\n",
        "                report = importance_df.sort_values(by='Importance', ascending=False)\n",
        "            else:\n",
        "                raise Exception\n",
        "\n",
        "            report[\"Feature_Name\"] = report[\"Feature\"].apply(report_modify)\n",
        "            report = report.reindex(columns=[\"Feature\",\"Feature_Name\",\"Importance\"])\n",
        "            report.to_csv(os.path.join(save_dir, f\"{target_column}-{dataset_name}-report-{model_name}.csv\"), index=False)\n",
        "        else:\n",
        "            report = None\n",
        "            print(\"No Valid Report for Current Model\")\n",
        "\n",
        "    return featurePath,targetPath,model, y_pred, report, model_fullname, cfc_report_dict, accuracy_num, gmeans_num, accuracy_num, roc_auc, best_threshold_num\n",
        "\n",
        "\n",
        "\n",
        "# Save the trained model and nan-value imputer\n",
        "def save_model(model, imputer, target_column, dataset_name, model_name, save_dir):\n",
        "    data = {\n",
        "    \"model\": model,\n",
        "    \"imputer\": imputer\n",
        "    }\n",
        "    with open(os.path.join(save_dir, f\"{target_column}-{dataset_name}-trained-{model_name}.pkl\"), 'wb') as file:\n",
        "        pickle.dump(data, file)\n",
        "\n",
        "# Modify the feature-importance report by adding an industry-correspondence introduction column\n",
        "def report_modify(value):\n",
        "    splitted = value.split(\"-\")\n",
        "    if splitted[0] in [\"Emp\",\"Est\",\"Pay\"]:\n",
        "        try:\n",
        "            modified = splitted[0]+\"-\"+INDUSTRIES_DICT[splitted[1]]+\"-\"+splitted[2]\n",
        "        except:\n",
        "            modified = value\n",
        "        return modified\n",
        "    else:\n",
        "        return value\n",
        "\n",
        "\n",
        "def report_generator(cfc_report_dict, model_fullname, model_name, gmeans_num, accuracy_num, roc_auc, best_threshold_num):\n",
        "    #transfrom report from dictionary to df\n",
        "    df_report = pd.DataFrame.from_dict(cfc_report_dict).transpose()\n",
        "\n",
        "    #adjust data display format for md and yaml\n",
        "    df_report['support'] = df_report['support'].astype(int)\n",
        "    df_report.iloc[:, 0:3] = df_report.iloc[:, 0:3].round(2)\n",
        "    df_report.iloc[2,0] = \" \"\n",
        "    df_report.iloc[2,1] = \" \"\n",
        "    df_report.iloc[2,3] = df_report.iloc[3,3]\n",
        "\n",
        "    #edit roc_auc format\n",
        "    roc_auc = roc_auc *100\n",
        "\n",
        "    #covert numpy float to python float for yaml display\n",
        "    roc_auc = roc_auc.item()\n",
        "    best_threshold_num = best_threshold_num.item()\n",
        "    gmeans_num = gmeans_num.item()\n",
        "\n",
        "    #markdown file content\n",
        "    markdown_content = f\"\"\"\n",
        "## {model_fullname} Accuracy\n",
        "\n",
        "**ROC-AUC Score:** {roc_auc}% &nbsp;&nbsp; **Best Threshold:** {best_threshold_num} &nbsp;&nbsp; **G-Mean:** {gmeans_num} &nbsp;&nbsp; **Model Accuracy:** {accuracy_num}%\n",
        "\n",
        "                    Precision   Recall      F1-Score    Support\n",
        "\n",
        "    0               {df_report.iloc[0,0]}        {df_report.iloc[0,1]}        {df_report.iloc[0,2]}        {df_report.iloc[0,3]}\n",
        "    1               {df_report.iloc[1,0]}        {df_report.iloc[1,1]}        {df_report.iloc[1,2]}        {df_report.iloc[1,3]}\n",
        "\n",
        "    Accuracy                                {df_report.iloc[2,2]}        {df_report.iloc[3,3]}\n",
        "    Macro Avg       {df_report.iloc[3,0]}        {df_report.iloc[3,1]}        {df_report.iloc[3,2]}        {df_report.iloc[3,3]}\n",
        "    Weighted Avg    {df_report.iloc[4,0]}        {df_report.iloc[4,1]}        {df_report.iloc[4,2]}        {df_report.iloc[3,3]}\n",
        "\"\"\"\n",
        "\n",
        "    #yaml output dictionary\n",
        "    report_dict = {\n",
        "    \"model_fullname\": model_fullname,\n",
        "    \"roc_auc\": roc_auc,\n",
        "    \"best_threshold_num\": best_threshold_num,\n",
        "    \"gmeans_num\": gmeans_num,\n",
        "    \"accuracy_num\": accuracy_num,\n",
        "    \"classification_report\": df_report.to_dict(orient=\"index\")\n",
        "    }\n",
        "\n",
        "    with open(f'{model_name}_accuracy.md','w') as markdown_file:\n",
        "        markdown_file.write(markdown_content)\n",
        "\n",
        "    with open(f'{model_name}_accuracy.yaml', \"w\") as f:\n",
        "        yaml.dump(report_dict, f, default_flow_style=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bglCZfAox9gP"
      },
      "outputs": [],
      "source": [
        "# Read the integrated full dataset and do the train-test splitting and save the splitted files\n",
        "if save_training:\n",
        "  integrated_df = pd.read_csv(os.path.join(full_save_dir, f\"{target_column}-{dataset_name}.csv\"))\n",
        "\n",
        "X_total, y_total = df.iloc[:, feature_start_idx:], df.iloc[:, target_idx] #X_total, y_total = integrated_df.iloc[:, feature_start_idx:], integrated_df.iloc[:, target_idx]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_total, y_total, test_size=0.2, random_state=random_state)\n",
        "X_train.to_csv(os.path.join(full_save_dir, f\"{target_column}-{dataset_name}-X-train.csv\"), index=False)\n",
        "X_test.to_csv(os.path.join(full_save_dir, f\"{target_column}-{dataset_name}-X-test.csv\"), index=False)\n",
        "y_train.to_csv(os.path.join(full_save_dir, f\"{target_column}-{dataset_name}-y-train.csv\"), index=False)\n",
        "y_test.to_csv(os.path.join(full_save_dir, f\"{target_column}-{dataset_name}-y-test.csv\"), index=False)\n",
        "\n",
        "if save_training:\n",
        "  file_path = os.path.join(full_save_dir, f\"X_train.csv\")\n",
        "  X_train.to_csv(file_path, index=False)\n",
        "\n",
        "  file_path = os.path.join(full_save_dir, f\"X_test.csv\")\n",
        "  X_test.to_csv(file_path, index=False)\n",
        "\n",
        "  file_path = os.path.join(full_save_dir, f\"y_train.csv\")\n",
        "  y_train.to_csv(file_path, index=False)\n",
        "\n",
        "  file_path = os.path.join(full_save_dir, f\"y_test.csv\")\n",
        "  y_test.to_csv(file_path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpmeBMiYx9gP"
      },
      "source": [
        "Model training, testing and results saving:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vqiy1m6-x9gQ"
      },
      "outputs": [],
      "source": [
        "# Training Random Forest\n",
        "featurePath, targetPath, model, y_pred, report, model_fullname, cfc_report_dict, accuracy_num, gmeans_num, accuracy_num, roc_auc, best_threshold_num= train(features_path ,targets_path,\"RandomForest\", target_column, dataset_name, X_train, y_train, X_test, y_test,\n",
        "      report_gen=True, all_model_list=all_model_list, valid_report_list=valid_report_list, over_sample=False, model_saving=True, random_state=random_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKhOoFlz2rwj"
      },
      "outputs": [],
      "source": [
        "# Generate markdown and yaml file\n",
        "# Results will appear in the content folder to the left\n",
        "report_generator(cfc_report_dict, model_fullname, \"RandomForest\", gmeans_num, accuracy_num, roc_auc, best_threshold_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmFTVDnjx9gS"
      },
      "outputs": [],
      "source": [
        "# Generating dummy values to handle the categorical columns: 'Population-2018', 'Population-2019', 'Population-2020'\n",
        "X_train = pd.get_dummies(X_train)\n",
        "X_test = pd.get_dummies(X_test)\n",
        "X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
        "\n",
        "#Train XGBoost model\n",
        "featurePath,targetPath, model, y_pred, report, model_fullname, cfc_report_dict, accuracy_num, gmeans_num, accuracy_num, roc_auc, best_threshold_num  = train(features_path, targets_path, \"XGBoost\", target_column, dataset_name, X_train, y_train, X_test, y_test,\n",
        "      report_gen=True, all_model_list=all_model_list, valid_report_list=valid_report_list, over_sample=False, model_saving=True, random_state=random_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jQDQdxbx9gS"
      },
      "outputs": [],
      "source": [
        "# Generate Report for XGBoost\n",
        "report_generator(cfc_report_dict, \"XGBoost\", \"XGBoost\", gmeans_num, accuracy_num, roc_auc, best_threshold_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sisc8cmSnMJU"
      },
      "outputs": [],
      "source": [
        "# Load realitystream/models/rbf.py\n",
        "import realitystream.models.rbf as rbfX\n",
        "rbfX.runrbf(param)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}