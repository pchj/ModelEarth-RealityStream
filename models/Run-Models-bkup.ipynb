{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Merge with Target (includes XGBoost)\n",
        "\n",
        "Add documentation\n",
        "https://model.earth/RealityStream/input/industries/\n",
        "\n",
        "Save and run locally: ../../models/Feature-Target-XGBoost-bkup.ipynb\n",
        "\n",
        "TO DO: Load from parameters.yaml file  \n",
        "TO DO: Use Pandas rather than saving training files (save some toy csv files)   \n",
        "TO DO: Add a path parameter that pulls from [all-years](https://colab.research.google.com/drive/1zu0WcCiIJ5X3iN1Hd1KSW4dGn0JuodB8#scrollTo=jxZiI7xcrT4B) generated by our [Industry Features CoLab](https://colab.research.google.com/drive/1HJnuilyEFjBpZLrgxDa4S0diekwMeqnh?usp=sharing)   \n",
        "TO DO: Allow parameters.yaml path to be set in the left side of CoLab. Retain a default path. Add How To update the left side here.  \n",
        "TO DO: From Merge-with-Target-XGBoost-bkup.ipynb load targets from Google Data Commons by calling a separate python file. Match targets on the FIPS value (county or state)\n",
        "https://github.com/ModelEarth/community-timelines/tree/main/training/all-years"
      ],
      "metadata": {
        "id": "jxZiI7xcrT4B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtFKCS4LkOzN"
      },
      "outputs": [],
      "source": [
        " import pandas as pd\n",
        "import regex as re\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_curve, roc_auc_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import xgboost as xgb\n",
        "from xgboost import plot_importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRc8O8CvkYx1"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"bees\" # Specify the target dataset name\n",
        "model_name = \"RandomForest\" # Specify the model to be trained\n",
        "all_model_list = [\"LogisticRegression\", \"SVM\", \"MLP\", \"RandomForest\", \"XGBoost\"] # All usable models\n",
        "assert model_name in all_model_list\n",
        "valid_report_list = [\"RandomForest\", \"XGBoost\"] # All valid models to generate feature-importance report\n",
        "\n",
        "random_state = 42 # Specify random state\n",
        "\n",
        "# Feature related information:\n",
        "country = \"US\"\n",
        "years = range(2017,2022)\n",
        "naics_level = 2\n",
        "naics_list = [2,4,6]\n",
        "assert naics_level in naics_list\n",
        "\n",
        "# Target related information:\n",
        "target_url = f\"https://raw.githubusercontent.com/ModelEarth/RealityStream/main/input/{dataset_name}/targets/{dataset_name}-targets.csv\"\n",
        "target_df = pd.read_csv(target_url) # Get the target csv\n",
        "\n",
        "target_column = '2022_increase' # Specify the target column\n",
        "target_list = ['2007_increase','2012_increase','2017_increase','2022_increase'] # Specify all usable target columns\n",
        "target_list.remove(target_column) # Drop the one we are interested in\n",
        "\n",
        "year_list = [\"2002\",\"2007\",\"2012\",\"2017\",\"2022\"]\n",
        "drop_list = ['Unnamed: 0','Name','State','State ANSI', 'County ANSI', \"Ag District\", \"Ag District Code\"]\n",
        "all_drop_list = drop_list + target_list + year_list # Drop all columns that can affect the training procedure or are not related\n",
        "\n",
        "feature_start_idx = 3 # Specify the starting column index in dataset csv for features, where first few columns are for target and id related stuff\n",
        "target_idx = 0 # Specify the column index for target\n",
        "\n",
        "# Directory Information:\n",
        "merged_save_dir = f\"../process/{dataset_name}/states-{target_column}-{dataset_name}\" # Specify the saving dir for state-separate dataset\n",
        "full_save_dir = f\"../output/{dataset_name}/training\" # Specify the saving dir for the integrated dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdUt24w63WDa"
      },
      "outputs": [],
      "source": [
        "# Get Dictionaries for state-correspondence and industry-correspondence\n",
        "\n",
        "STATE_DICT = {\n",
        "    \"AL\": \"ALABAMA\",\n",
        "    \"AK\": \"ALASKA\",\n",
        "    \"AZ\": \"ARIZONA\",\n",
        "    \"AR\": \"ARKANSAS\",\n",
        "    \"CA\": \"CALIFORNIA\",\n",
        "    \"CO\": \"COLORADO\",\n",
        "    \"CT\": \"CONNECTICUT\",\n",
        "    \"DE\": \"DELAWARE\",\n",
        "    \"FL\": \"FLORIDA\",\n",
        "    \"GA\": \"GEORGIA\",\n",
        "    \"HI\": \"HAWAII\",\n",
        "    \"ID\": \"IDAHO\",\n",
        "    \"IL\": \"ILLINOIS\",\n",
        "    \"IN\": \"INDIANA\",\n",
        "    \"IA\": \"IOWA\",\n",
        "    \"KS\": \"KANSAS\",\n",
        "    \"KY\": \"KENTUCKY\",\n",
        "    \"LA\": \"LOUISIANA\",\n",
        "    \"ME\": \"MAINE\",\n",
        "    \"MD\": \"MARYLAND\",\n",
        "    \"MA\": \"MASSACHUSETTS\",\n",
        "    \"MI\": \"MICHIGAN\",\n",
        "    \"MN\": \"MINNESOTA\",\n",
        "    \"MS\": \"MISSISSIPPI\",\n",
        "    \"MO\": \"MISSOURI\",\n",
        "    \"MT\": \"MONTANA\",\n",
        "    \"NE\": \"NEBRASKA\",\n",
        "    \"NV\": \"NEVADA\",\n",
        "    \"NH\": \"NEW HAMPSHIRE\",\n",
        "    \"NJ\": \"NEW JERSEY\",\n",
        "    \"NM\": \"NEW MEXICO\",\n",
        "    \"NY\": \"NEW YORK\",\n",
        "    \"NC\": \"NORTH CAROLINA\",\n",
        "    \"ND\": \"NORTH DAKOTA\",\n",
        "    \"OH\": \"OHIO\",\n",
        "    \"OK\": \"OKLAHOMA\",\n",
        "    \"OR\": \"OREGON\",\n",
        "    \"PA\": \"PENNSYLVANIA\",\n",
        "    \"RI\": \"RHODE ISLAND\",\n",
        "    \"SC\": \"SOUTH CAROLINA\",\n",
        "    \"SD\": \"SOUTH DAKOTA\",\n",
        "    \"TN\": \"TENNESSEE\",\n",
        "    \"TX\": \"TEXAS\",\n",
        "    \"UT\": \"UTAH\",\n",
        "    \"VT\": \"VERMONT\",\n",
        "    \"VA\": \"VIRGINIA\",\n",
        "    \"WA\": \"WASHINGTON\",\n",
        "    \"WV\": \"WEST VIRGINIA\",\n",
        "    \"WI\": \"WISCONSIN\",\n",
        "    \"WY\": \"WYOMING\"\n",
        "}\n",
        "\n",
        "try:\n",
        "    industries_df = pd.read_csv(f\"https://raw.githubusercontent.com/ModelEarth/community-data/master/{country.lower()}/id_lists/naics{naics_level}.csv\",header=None)\n",
        "    INDUSTRIES_DICT = industries_df.set_index(0).to_dict()[1]\n",
        "except:\n",
        "    INDUSTRIES_DICT = dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jv_AUQwjnrkN"
      },
      "outputs": [],
      "source": [
        "def rename_columns(df, year):\n",
        "    rename_mapping = {}\n",
        "    for column in df.columns:\n",
        "      if column not in df.columns[:2]:\n",
        "          new_column_name = column + f'-{year}'\n",
        "          rename_mapping[column] = new_column_name\n",
        "\n",
        "    df.rename(columns=rename_mapping, inplace=True)\n",
        "\n",
        "def check_directory(directory_path): # Check whether the given directory exists, if not, then create it\n",
        "    if not os.path.exists(directory_path):\n",
        "        try:\n",
        "            os.makedirs(directory_path)\n",
        "            print(f\"Directory '{directory_path}' created successfully.\")\n",
        "        except OSError as e:\n",
        "            print(f\"Error creating directory '{directory_path}': {e}\")\n",
        "    else:\n",
        "        print(f\"Directory '{directory_path}' already exists.\")\n",
        "    return directory_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kd5ZXB8M2Sp6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b94bac3f-be7c-45b4-ac4e-306ab20e96ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory '../process/bees/states-2022_increase-bees' created successfully.\n"
          ]
        }
      ],
      "source": [
        "check_directory(merged_save_dir)\n",
        "\n",
        "# State-separately, for each state, merging industry features and target on Fips value and County Name, return the merged csv\n",
        "\n",
        "for state in STATE_DICT:\n",
        "    data = {}\n",
        "    for year in years:\n",
        "        url = f\"https://raw.githubusercontent.com/ModelEarth/community-timelines/main/training/naics{naics_level}/{country}/counties/{year}/{country}-{state}-training-naics{naics_level}-counties-{year}.csv\"\n",
        "        data[year] = pd.read_csv(url)\n",
        "        rename_columns(data[year], year)\n",
        "\n",
        "    merged_df_feature = pd.merge(data[2017], data[2018], on=['Fips', 'Name'], how='inner')\n",
        "\n",
        "    for year in range(2019,2022):\n",
        "        merged_df_feature = pd.merge(merged_df_feature, data[year], on=['Fips', 'Name'], how='inner')\n",
        "\n",
        "    cols = merged_df_feature.columns.tolist()\n",
        "    cols = cols[:2] + sorted(cols[2:])\n",
        "    merged_df_feature = merged_df_feature[cols].rename(columns={\"Name\": \"County\"})\n",
        "\n",
        "    merged_df = pd.merge(merged_df_feature, target_df[target_df[\"State\"]==STATE_DICT[state]], on=[\"Fips\",\"County\"], how=\"inner\")\n",
        "    merged_df.drop(columns=all_drop_list, axis=1, inplace=True)\n",
        "\n",
        "    target = merged_df.iloc[:, -1]\n",
        "    merged_df.drop(columns=[target_column], axis=1, inplace=True)\n",
        "    merged_df.insert(0, 'target', target)\n",
        "\n",
        "    merged_df.to_csv(os.path.join(merged_save_dir, f\"{state}-{target_column}-{dataset_name}.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoHLYF61xyl1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b19d9fad-ed25-4cc5-f461-a320ef37a0e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory '../output/bees/training' created successfully.\n"
          ]
        }
      ],
      "source": [
        "# Integrate all the state-separate csv files into one, return the full dataset csv\n",
        "\n",
        "check_directory(full_save_dir)\n",
        "\n",
        "dataframes = []\n",
        "csv_directory = f\"../process/{dataset_name}/states-{target_column}-{dataset_name}\"\n",
        "csv_files = os.listdir(csv_directory)\n",
        "for csv_file in csv_files:\n",
        "    if csv_file.endswith('.csv'):\n",
        "        dataframes.append(pd.read_csv(os.path.join(csv_directory, csv_file)))\n",
        "\n",
        "integrated_df = pd.concat(dataframes, ignore_index=True)\n",
        "integrated_df.to_csv(os.path.join(full_save_dir, f\"{target_column}-{dataset_name}.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SxJJr6-x9gO"
      },
      "outputs": [],
      "source": [
        "# Train the model and get the test report\n",
        "def train_model(model, X_train, y_train, X_test, y_test, over_sample):\n",
        "    if over_sample:\n",
        "        sm = SMOTE(random_state = 2)\n",
        "        X_train, y_train = sm.fit_resample(X_train, y_train.ravel())\n",
        "        print(\"Oversampling Done for Training Data.\")\n",
        "\n",
        "    model = model.fit(X_train, y_train)\n",
        "    print(\"Model Fitted Successfully.\")\n",
        "\n",
        "    # calculating y_pred\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_prob = model.predict_proba(X_test)\n",
        "    roc_auc = round(roc_auc_score(y_test, y_pred_prob[:, 1]), 2)\n",
        "\n",
        "    print(f\"\\033[1mROC-AUC Score\\033[0m \\t\\t: {roc_auc*100} %\")\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob[:,1], pos_label=1)\n",
        "\n",
        "    gmeans = np.sqrt(tpr * (1-fpr))\n",
        "\n",
        "    ix = np.argmax(gmeans)\n",
        "    print('\\033[1mBest Threshold\\033[0m \\t\\t: %.3f \\n\\033[1mG-Mean\\033[0m \\t\\t\\t: %.3f' % (thresholds[ix], gmeans[ix]))\n",
        "\n",
        "    y_pred = (y_pred > thresholds[ix])\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"\\033[1mModel Accuracy\\033[0m \\t\\t:\", round(accuracy,2,)*100, \"%\")\n",
        "\n",
        "    print(\"\\033[1m\\nClassification Report:\\033[0m\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    return model, y_pred\n",
        "\n",
        "# Train the specified model, impute the nan values, and save the trained model as well as the feature-target report\n",
        "def train(model_name, target_column, dataset_name, X_train, y_train, X_test, y_test, report_gen, all_model_list, valid_report_list, over_sample=False, model_saving=True, random_state=42):\n",
        "    assert model_name in all_model_list\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_train_imputed = imputer.fit_transform(X_train)\n",
        "    X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "    if model_name == \"LogisticRegression\":\n",
        "        model = LogisticRegression(max_iter=10000, random_state=random_state)\n",
        "    elif model_name == \"SVM\":\n",
        "        model = SVC(random_state=random_state,probability=True)\n",
        "    elif model_name == \"MLP\":\n",
        "        model = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=1000, random_state=random_state)\n",
        "    elif model_name == \"RandomForest\":\n",
        "        model = RandomForestClassifier(n_jobs=3, n_estimators=1000, criterion=\"gini\", random_state=random_state)\n",
        "    elif model_name == \"XGBoost\":\n",
        "        model = xgb.XGBClassifier(random_state=random_state)\n",
        "    else:\n",
        "        raise Exception\n",
        "\n",
        "    if model_name == \"XGBoost\":\n",
        "        model, y_pred = train_model(model, X_train, y_train, X_test, y_test, over_sample) # No need to impute nan values for XGBoost\n",
        "    else:\n",
        "        model, y_pred = train_model(model, X_train_imputed, y_train, X_test_imputed, y_test, over_sample)\n",
        "\n",
        "    save_dir = f\"../output/{dataset_name}/saved\"\n",
        "    check_directory(save_dir)\n",
        "\n",
        "    if model_saving:\n",
        "        if model_name == \"XGBoost\":\n",
        "            save_model(model, None, target_column, dataset_name, model_name, save_dir) # No need to impute nan values for XGBoost\n",
        "        else:\n",
        "            save_model(model, imputer, target_column, dataset_name, model_name, save_dir)\n",
        "\n",
        "    if report_gen:\n",
        "        if model_name in valid_report_list:\n",
        "            if model_name == \"RandomForest\":\n",
        "                importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': model.feature_importances_})\n",
        "                report = importance_df.sort_values(by='Importance', ascending=False)\n",
        "            elif model_name == \"XGBoost\":\n",
        "                importance_df = pd.DataFrame(list(model.get_booster().get_score().items()), columns=[\"Feature\",\"Importance\"])\n",
        "                report = importance_df.sort_values(by='Importance', ascending=False)\n",
        "            else:\n",
        "                raise Exception\n",
        "\n",
        "            report[\"Feature_Name\"] = report[\"Feature\"].apply(report_modify)\n",
        "            report = report.reindex(columns=[\"Feature\",\"Feature_Name\",\"Importance\"])\n",
        "            report.to_csv(os.path.join(save_dir, f\"{target_column}-{dataset_name}-report-{model_name}.csv\"), index=False)\n",
        "        else:\n",
        "            report = None\n",
        "            print(\"No Valid Report for Current Model\")\n",
        "\n",
        "    return model, y_pred, report\n",
        "\n",
        "# Save the trained model and nan-value imputer\n",
        "def save_model(model, imputer, target_column, dataset_name, model_name, save_dir):\n",
        "    data = {\n",
        "    \"model\": model,\n",
        "    \"imputer\": imputer\n",
        "    }\n",
        "    with open(os.path.join(save_dir, f\"{target_column}-{dataset_name}-trained-{model_name}.pkl\"), 'wb') as file:\n",
        "        pickle.dump(data, file)\n",
        "\n",
        "# Modify the feature-importance report by adding an industry-correspondence introduction column\n",
        "def report_modify(value):\n",
        "    splitted = value.split(\"-\")\n",
        "    if splitted[0] in [\"Emp\",\"Est\",\"Pay\"]:\n",
        "        try:\n",
        "            modified = splitted[0]+\"-\"+INDUSTRIES_DICT[splitted[1]]+\"-\"+splitted[2]\n",
        "        except:\n",
        "            modified = value\n",
        "        return modified\n",
        "    else:\n",
        "        return value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bglCZfAox9gP"
      },
      "outputs": [],
      "source": [
        "# Read the integrated full dataset and do the train-test splitting and save the splitted files\n",
        "integrated_df = pd.read_csv(os.path.join(full_save_dir, f\"{target_column}-{dataset_name}.csv\"))\n",
        "X_total, y_total = integrated_df.iloc[:, feature_start_idx:], integrated_df.iloc[:, target_idx]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_total, y_total, test_size=0.2, random_state=random_state)\n",
        "X_train.to_csv(os.path.join(full_save_dir, f\"{target_column}-{dataset_name}-X-train.csv\"), index=False)\n",
        "X_test.to_csv(os.path.join(full_save_dir, f\"{target_column}-{dataset_name}-X-test.csv\"), index=False)\n",
        "y_train.to_csv(os.path.join(full_save_dir, f\"{target_column}-{dataset_name}-y-train.csv\"), index=False)\n",
        "y_test.to_csv(os.path.join(full_save_dir, f\"{target_column}-{dataset_name}-y-test.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpmeBMiYx9gP"
      },
      "source": [
        "Model training, testing and results saving:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vqiy1m6-x9gQ",
        "outputId": "a1b94fd2-ccca-40d8-acf9-595b8cd48003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Fitted Successfully.\n",
            "\u001b[1mROC-AUC Score\u001b[0m \t\t: 56.00000000000001 %\n",
            "\u001b[1mBest Threshold\u001b[0m \t\t: 0.632 \n",
            "\u001b[1mG-Mean\u001b[0m \t\t\t: 0.562\n",
            "\u001b[1mModel Accuracy\u001b[0m \t\t: 68.0 %\n",
            "\u001b[1m\n",
            "Classification Report:\u001b[0m\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.41      0.09      0.15       177\n",
            "         1.0       0.70      0.94      0.80       393\n",
            "\n",
            "    accuracy                           0.68       570\n",
            "   macro avg       0.55      0.52      0.47       570\n",
            "weighted avg       0.61      0.68      0.60       570\n",
            "\n",
            "Directory '../output/bees/saved' created successfully.\n"
          ]
        }
      ],
      "source": [
        "model, y_pred, report = train(model_name, target_column, dataset_name, X_train, y_train, X_test, y_test,\n",
        "      report_gen=True, all_model_list=all_model_list, valid_report_list=valid_report_list, over_sample=False, model_saving=True, random_state=random_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmFTVDnjx9gS",
        "outputId": "c268b001-f3a3-468c-b253-c0a803f8cdf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Fitted Successfully.\n",
            "\u001b[1mROC-AUC Score\u001b[0m \t\t: 56.00000000000001 %\n",
            "\u001b[1mBest Threshold\u001b[0m \t\t: 0.632 \n",
            "\u001b[1mG-Mean\u001b[0m \t\t\t: 0.562\n",
            "\u001b[1mModel Accuracy\u001b[0m \t\t: 68.0 %\n",
            "\u001b[1m\n",
            "Classification Report:\u001b[0m\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.41      0.09      0.15       177\n",
            "         1.0       0.70      0.94      0.80       393\n",
            "\n",
            "    accuracy                           0.68       570\n",
            "   macro avg       0.55      0.52      0.47       570\n",
            "weighted avg       0.61      0.68      0.60       570\n",
            "\n",
            "Directory '../output/bees/saved' already exists.\n"
          ]
        }
      ],
      "source": [
        "model, y_pred, report = train(\"RandomForest\", target_column, dataset_name, X_train, y_train, X_test, y_test,\n",
        "      report_gen=True, all_model_list=all_model_list, valid_report_list=valid_report_list, over_sample=False, model_saving=True, random_state=random_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jQDQdxbx9gS",
        "outputId": "7f683791-e723-4e23-c997-7db782dbf9e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Fitted Successfully.\n",
            "\u001b[1mROC-AUC Score\u001b[0m \t\t: 51.0 %\n",
            "\u001b[1mBest Threshold\u001b[0m \t\t: 0.724 \n",
            "\u001b[1mG-Mean\u001b[0m \t\t\t: 0.511\n",
            "\u001b[1mModel Accuracy\u001b[0m \t\t: 61.0 %\n",
            "\u001b[1m\n",
            "Classification Report:\u001b[0m\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.31      0.23      0.26       177\n",
            "         1.0       0.69      0.78      0.73       393\n",
            "\n",
            "    accuracy                           0.61       570\n",
            "   macro avg       0.50      0.50      0.50       570\n",
            "weighted avg       0.57      0.61      0.59       570\n",
            "\n",
            "Directory '../output/bees/saved' already exists.\n"
          ]
        }
      ],
      "source": [
        "model, y_pred, report = train(\"XGBoost\", target_column, dataset_name, X_train, y_train, X_test, y_test,\n",
        "      report_gen=True, all_model_list=all_model_list, valid_report_list=valid_report_list, over_sample=False, model_saving=True, random_state=random_state)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}